---

layout: post
title: A Modular System for Assistive and Interactive Cooking
date: 2025-09-01 03:28:17
authors: [Xuanhao Song, Cong Fu]
comments: true
toc: true
categories: [Projects]
tag: [LLM, VLM, HRI, intention]
---

### Repository Link: https://github.com/Haoqingtianxia1997/Mosaic 

---

## **1. Introduction**

Interactive robot arm manipulation can be achieved through various approaches. While end-to-end models have shown promise, at the current stage they often struggle with generalizability across diverse tasks and environments. In contrast, a hierarchical architecture offers a robust alternative [1]: by decoupling perception and control, it enables reliable manipulation. Incorporating vision-language models (VLMs) into the perception module further enhances generalization across diverse scenarios. Furthermore, long-horizon tasks can be decomposed into manageable subtasks, with action sequences dynamically arranged by large language models (LLMs).

Modern human–robot interaction increasingly leverages multiple modalities, reflecting the way humans naturally communicate through speech, gaze, and gestures. By integrating these diverse cues—such as human gaze and gestures—alongside audio input (processed via speech-to-text), the robot system can infer user intentions more accurately and respond with context-appropriate action sequences generated by the LLM. Building on this capability, we explore the integration of additional interaction modalities—such as human gaze and gestures—alongside audio input. By fusing these multimodal cues, the robot system can infer user intentions more accurately and respond with context-appropriate action sequences generated by the LLM. Our vision is to develop a system that empowers robots to serve as intelligent assistants in everyday household scenarios, seamlessly collaborating with humans through natural and robust interaction. 

---

## **2. Software Stack**
To enable natural human-robot interaction and efficient robot control, we designed a **software stack** that supports the entire system. The architecture consists of five main layers: **Multimodal Interaction**, **Decision Making**, **Robot Control**, **Intention Modules**, and **Perception**.

### **2.1 Multimodal Interaction**

At the interaction layer, our system leverages multiple modalities to enable natural and robust human–robot communication. 

- **Voice (Whisper)** [2]: The speech interface processes natural voice commands from the user and converts them into text, enabling voice-driven interaction. This allows the user to control the robot directly through spoken instructions. 
- **Gesture (Mediapipe)**: The gesture module detects and interprets hand and body gestures to complement voice input, providing additional context for the robot to understand user intent.
- **Gaze (L2CS-Net)** [3]: The gaze module estimates the user’s gaze direction, helping the robot infer the user’s focus of attention and further enhancing intention recognition.

By fusing these multimodal cues, the robot can not only “listen” to instructions but also “read” the user’s intent, resulting in more accurate and context-appropriate responses.

### **2.2 Decision Making**

For decision making, we adopt a **Large Language Model (LLM)** to handle command interpretation and task planning. Its main responsibilities include:

- Understanding and structuring user input

- Extracting key information from natural language and formulate robot actions in structured JSON format

- Sorting and generating action sequences dynamically based on context

This layer essentially acts as the robot’s “brain,” bridging the gap between human language and executable robotic decisions.

### **2.3 Robot Control**

The robot control components translate high-level plans into low-level control commands for the robot, including:

- **ROS2 Humble Service**: calling custom services to execute predefined elementary actions

- **MoveIt2**: serving as the robot’s kinematic solver for path planning and motion execution

With the help of these components, the abstract actions from the decision module can be executed seamlessly by the robot hardware.

### **2.4 Perception**

The perception layer focuses on understanding the environment to support human-robot interaction:

- **VLM**: finds the object described by the user and outputs object information in JSON format

- **OWL-ViT** [4]: enables zero-shot object detection from textual descriptions, producing bounding boxes

- **SAM** (Segment Anything Model) [5]: generates segmentation masks for precise object recognition

Together, these models equip the robot with multimodal perception, enabling it to understand both language and vision.

---

## **3 Implementation**

### **3.1 High-level decision-making perception layer**

#### **3.1.1 Interaction and decision-making**

Voice interaction with the system is achieved by whisper for speech to text (STT) and Google’s text to speech (TTS, requires internet connection). When a text input from voice command is given to the system, it’ll go through intention analysis module (more on that later). Then LLM agent will sort the action sequence, give user a response to confirm the command after inference, and then extract action type, target and parameters upon text commands. Such approach will enable the actions to be ordered without maintaining a handcrafted behavior tree and use LLM’s reasoning to handle the sequence by prompt engineering. Each action in the JSON will call ros2 services that we’ve created to execute the corresponding actions. 

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.1 Interaction and decision-make framework.png"
       alt="Fig.1 Interaction and decision-make framework"
       width="700">
  <figcaption><strong>Fig.1 Interaction and decision-make framework</strong></figcaption>
</figure>

#### **3.1.2 Perception**

We’ve implemented high level and low level perception in our system. As part of the action type, perceive in high level perception framework is achieved by the combination of VLM, detection and segmentation. 

1. **Input and task formulation**: When the user issues a command, it is first translated into a structured query. For instance, the system may create a JSON-like request such as:

    ```json
    { "type": "perceive", "target": "cucumber", "parameters": {} }
    ```
    
    This query specifies the target object the system should search for.

2.	**VLM agent processing**: The query, along with the current image of the scene, is passed to the VLM agent. The agent attempts to locate the target object within the environment.

3.	**Decision branch — object found or not**:

      - If the object is not found, the system terminates the perception process and triggers TTS (Text-to-Speech) to provide user feedback, gracefully ending the action sequence.


      - If the object is found, the system generates a more detailed and precise prompt based on the detection result. For example, instead of a generic “cucumber,” the refined description might become “a green cucumber”.

4.	**Output to downstream modules**: The refined prompt is then forwarded to the low-level perception module, where more accurate detection, segmentation, and 3D mapping can take place.
In summary, the high-level perception framework acts as a filter and refiner of user intent, bridging abstract natural language commands with detailed visual queries that can be reliably grounded in the physical world.

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.2 High-level perception framework.png"
       alt="Fig.2 High-level perception framework"
       width="700">
  <figcaption><strong>Fig.2 High-level perception framework</strong></figcaption>
</figure>

In the low-level perception module, the system must transform natural language descriptions into viable 3D target points in the real world. This process unfolds through several stages:

1. **From high-level perception to refined input**: The user’s natural language command first goes through the high-level perception layer (VLM), which analyzes the input and produces a more accurate and specific textual prompt. For example, a spoken command like “give me the cucumber” might be refined into “a green cucumber”, making it easier for downstream vision models to correctly identify the target.

2. **Initial detection**: The refined prompt is then passed to OWL-ViT, which performs zero-shot detection and returns a coarse bounding box of the object in the scene.

3. **Segmentation and fine-grained processing**: The detected region is sent to Segment Anything Model (SAM), which extracts the object’s precise segmentation mask and center point. This step ensures pixel-level accuracy in object localization.

4. **Camera calibration and 3D mapping**: Using the intrinsic and extrinsic parameters of the camera, the segmented pixel coordinates are projected into 3D point clouds, effectively bridging the 2D image domain with the robot’s operational 3D workspace.

5. **Point cloud preprocessing**: To make the point cloud usable, several operations are applied:

   - Voxel downsampling to reduce redundancy and improve efficiency

   - Radius filtering to remove isolated noise points

   - DBSCAN clustering to separate distinct objects

   - ICP (Iterative Closest Point) to further align and refine the cloud

  6.	**Target point generation**: Finally, the system produces the target object’s point cloud along with a corresponding world-coordinate goal point, which serves as the foundation for grasping and downstream manipulation strategies.

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.3 Low-level perception framework.png"
       alt="Fig.3 Low-level perception framework"
       width="700">
  <figcaption><strong>Fig.3 Low-level perception framework</strong></figcaption>
</figure>

Apart from OWL-ViT for zero shot detection, a yolo model has also been trained from the objects used in our scene as an alternative object detection method. We’ve taken photos for single objects as well as for multiple object combinations, augmented the raw dataset and trained the model with yolov11 [6]. The training results are shown as follows. The objects of interest are tested with high confidence score. 

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.4 Validation results.jpg"
       alt="Fig.4 Validation results"
       width="700">
  <figcaption><strong>Fig.4 Validation results</strong></figcaption>
</figure>

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.5 Confusion matrix.png"
       alt="Fig.5 Confusion matrix"
       width="700">
  <figcaption><strong>Fig.5 Confusion matrix</strong></figcaption>
</figure>

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.6 F1 Confidence score.png"
       alt="Fig.6 F1 Confidence score"
       width="700">
  <figcaption><strong>Fig.6 F1 Confidence score</strong></figcaption>
</figure>

### **3.2 Intention module**

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.8 Intent recognition module diagram.png"
       alt="Fig.8 Intent recognition module diagram"
       width="700">
  <figcaption><strong>Fig.8 Intent recognition module diagram</strong></figcaption>
</figure>

As illustrated in the figure, the intention recognition module determines the user’s focus area in the workspace by combining gaze and gesture information. The system captures the user’s gaze vector (red ray) and gesture vector (green ray) using vision-based tracking. Both vectors are extended into 3D space, and their intersection point on the table plane is calculated. This intersection region is interpreted as the candidate location of the user’s intention. Compared to unimodal methods, this multimodal fusion approach provides higher robustness. Gaze alone is often affected by noise and small drifts, while gesture alone may suffer from jitter or occlusion. By combining both modalities, the intersection constraint effectively reduces ambiguity and enables more accurate and stable intention inference. he identified target region is then passed to the high-level perception and task p nning modules, allowing the robot to align its actions with the user’s true intention. This method enhances the naturalness and reliability of human–robot interaction and serves as a key foundation for intuitive collaboration.
#### **3.2.1 Intention Module – Gaze-based 3D Vector Estimation**

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.9 Gaze-based 3D Vector Estimation.png"
       alt="Fig.9 Gaze-based 3D Vector Estimation"
       width="700">
  <figcaption><strong>Fig.9 Gaze-based 3D Vector Estimation</strong></figcaption>
</figure>

The intention module uses gaze information to estimate a 3D gaze vector, which is then mapped into the world coordinate frame. The figure illustrates the main steps:

1.	**Orientation Estimation**: 
The pitch and yaw angles of the gaze are estimated using L2CS-Net, providing the gaze direction in 3D space.

2.	**Origin Mapping**: 
The 2D gaze origin from the image coordinate system is back-projected into the camera coordinate frame and then transformed into the world frame.

3.	**Integration with Point Cloud**: 
The resulting 3D gaze vector is aligned with the scene’s point cloud to predict the region of interest (ROI) where the gaze intersects with the environment.

4.	**Object Detection in ROI**: 
Within the predicted ROI, an object detection model (e.g., YOLO) is applied to recognize the specific object being observed. By combining gaze estimation with object detection, the system achieves robust inference of user intention.

In summary, the gaze-based intention module transforms raw eye-tracking data into actionable world-frame information, which can be further combined with gestures or voice commands to enable natural and reliable human–robot interaction.

#### **3.2.2 Intention Module – Gesture-based 3D Vector Estimation**

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.10 Gesture-based 3D Vector Estimation.png"
       alt="Fig.10 Gesture-based 3D Vector Estimation"
       width="700">
  <figcaption><strong>Fig.10 Gesture-based 3D Vector Estimation</strong></figcaption>
</figure>

The intention module also leverages gesture information to infer the user’s focus area. Using vision-based hand skeleton detection, the system extracts the start and end points of the index finger to construct a 3D gesture vector. The process consists of the following steps:

1. **Orientation and Origin Estimation**:
With the Mediapipe framework, the 2D coordinates of the index finger joints are detected. These are then back-projected into 3D, providing both the origin and orientation of the gesture vector.

2. **Integration with Point Cloud**:
The gesture vector is projected into the scene’s point cloud, allowing the system to estimate the region of interest (ROI) that the user is pointing to.

3. **Object Detection in ROI**:
Within the predicted ROI, an object detection model (e.g., YOLO) is applied to identify the specific object being indicated.

This gesture-based approach enables intuitive and natural intention recognition. When combined with gaze information, it further improves the robustness and precision of user intention inference.

#### **3.2.3 Intention Module – Multimodal Fusion Pipeline**

The core function of the intention module is to integrate multimodal user inputs—voice, gaze, and gesture—into a unified intention command that can be executed by the robot. The process is divided into three main nodes:

1. **Voice Node (Node 1)**:
The system continuously listens for spoken input. When a new voice command is detected, the transcription and a Boolean flag are published; otherwise, the default state remains False.

2. **Perception Node (Node 2)**:
This node subscribes to RGB and depth images to compute the intersection of the gaze vector and the gesture vector. If the intersection is stable (i.e., not wobbling significantly), YOLO detection is applied in the region of interest (ROI) around the intersection point, and two object labels are published. If the intersection is unstable, None is returned.

3. **Fusion Node (Node 3)**:
This node combines the voice transcription with gaze- and gesture-based labels. If a new transcription is detected, both the voice command and the labels are passed to the Intention LLM, which performs a comprehensive analysis and generates the final intention command. If no new transcription is available, the system remains idle.

The resulting intention command is then forwarded to the Planning LLM Agent, which sorts the required actions and provides voice feedback, enabling the robot to execute the intended task.

By fusing voice, gaze, and gesture information, this module ensures robust and natural intention recognition, significantly enhancing the reliability of human–robot interaction.

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.11 Multimodal intent fusion process.png"
       alt="Fig.11 Multimodal intent fusion process"
       width="700">
  <figcaption><strong>Fig.11 Multimodal intent fusion process</strong></figcaption>
</figure>

### **3.3 Execution layer**
#### **3.3.1 Grasp strategy**
The grasp strategy aims to automatically generate multiple candidate grasp poses from the object’s point cloud and then select the most reliable one through collision checking and containment evaluation [7]. The overall workflow is illustrated in the figure:

1.	**Point Cloud Preprocessing**: The raw point cloud of the target object is first filtered and aligned, resulting in a clean 3D representation suitable for grasp analysis.

2.	**Bounding Box Generation**: An oriented bounding box (OBB) is constructed around the preprocessed point cloud. Its center and orientation define a reference frame for subsequent grasp pose sampling.

3.	**Grasp Pose Sampling** (Candidate grasps are sampled within the bounding box) :

    - Along the long axis, a Gaussian distribution is used, so grasp points are biased toward the center of the object for stability.

    - Along the short axis and vertical axis, uniform sampling ensures broad coverage.
    - The grasp coordinate system is defined as:

      * Z-axis pointing downward (approach direction of the gripper),

      * X-axis aligned with the object’s long axis (thickness direction of the gripper),

      * Y-axis aligned with the short axis (opening direction of the gripper).

    - Small random rotations around the Z-axis are added to introduce variability and robustness.

4. **Visualization of Candidates**: Each candidate grasp is represented by a 3D gripper model and visualized together with the target object’s point cloud, providing an     intuitive view of the sampling results.

5. **Collision Checking and Containment Evaluation** (All candidates are then filtered and scored):

    - Collision checking removes grasps that intersect with the object or the table.

    - Containment evaluation uses ray casting between the gripper fingers to determine whether the object can be effectively enclosed. Metrics such as containment ratio, intersection depth, and distance to the object’s centroid are combined into a weighted quality score.

6. **Selection of the Best Grasp**: The final grasp is chosen as the candidate that avoids collisions and achieves the highest quality score. Two poses are generated for execution: a pre-grasp pose (offset along the approach direction for safe entry) and the final grasp pose (closing the gripper on the object).

In summary, the grasp strategy follows a structured pipeline: point cloud → bounding box → pose sampling → evaluation → best grasp selection. This ensures that stable and feasible grasps can be identified even in complex environments.

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.7 Grasp strategy.png"
       alt="Fig.7 Grasp strategy"
       width="700">
  <figcaption><strong>Fig.7 Grasp strategy</strong></figcaption>
</figure>

#### **3.3.2 Motion control services**
Based on our needs from the tasks, we’ve written several ros2 custom services for primitive and complex motions to be called by generated commands from high level. They can be categorized into 4 parts:

1. **Fundamental Motions**:
   - **move (goal 6D pose)**: moves the robot to a specified 6D pose in joint space.
   - **move_cartesian (goal 6D pose)**: executes Cartesian motion to a target 6D pose.
   - **reset**: resets the robot to a safe default position.
2. **Gripper Control**:
   - **open**: opens the gripper.
   - **close**: closes the gripper with position and force control.
3. **Extended Motions**:
   - **add (times)**: call move service to move to the soup pot, then rotate the last joint (end effector) back and forth several times as representation to pour flavoring
   - **stir (center_x, center_y, center_z, radius, start_angle_deg, move_down_offset, speed, stir_time)**: executes a stirring action by moving to a preparation pose above the pot, lowering into the pot, and following a circular trajectory.
   - **return_back (prep 6D pose, place 6D pose)**: returns from a preparation pose to place the object back at its original position.
   - **grasp (prep 6D pose, grasp 6D pose)**: executes a complete grasp sequence, including moving to a preparation pose, transitioning to the grasp pose, closing the gripper, and lifting back to the preparation pose.
4. **Auxiliary Services**:
   - **image_saver**: saves RGB and depth images periodically (every 10 seconds).
   - **image_saver_yolo**: automatically saves YOLO detection results.
   - **forward_kinematic**: retrieves the current end-effector position through forward kinematics.

In summary, the execution-level module bridges perception and high-level planning with actual robot execution. By offering both primitive motion commands and task-specific macros (e.g., grasping, stirring, pouring), it ensures that complex cooking tasks can be reliably executed in a structured and reusable manner.

## **4 Demo**
### **4.1 Task with simple voice command**

<div style="max-width:700px;margin:0 auto;position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;">
  <iframe
    src="https://drive.google.com/file/d/1H1uiD9wTwBbZSxWQi2mnODs3Pw95SIGb/preview"
    allow="autoplay"
    style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
    allowfullscreen
  ></iframe>
</div>
<div style="height:50px;"></div>

### **4.2 Task with ambiguous voice command and detected intention**

<div style="max-width:700px;margin:0 auto;position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;">
  <iframe
    src="https://drive.google.com/file/d/14xdFmQEaCdJhPBztq6Ylszzb2Ji9D2mZ/preview"
    allow="autoplay"
    style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
    allowfullscreen
  ></iframe>
</div>
<div style="height:50px;"></div>


<div style="max-width:700px;margin:0 auto;position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;">
  <iframe
    src="https://drive.google.com/file/d/1S1qMOrox8GJC7Ebx33jQp7KKOY_THWGL/preview"
    allow="autoplay"
    style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
    allowfullscreen
  ></iframe>
</div>
<div style="height:50px;"></div>

### **4.3 Interaction and long horizon task planning**

<div style="max-width:700px;margin:0 auto;position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;">
  <iframe
    src="https://drive.google.com/file/d/12Mh2bDW-xWiww5Braki14RKYXPVPcFx_/preview"
    allow="autoplay"
    style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
    allowfullscreen
  ></iframe>
</div>
<div style="height:50px;"></div>


## **5 Summary and Outlook**

<figure style="text-align: center;">
  <img src="/assets/mosaic_imgs/Fig.12 scene setup.jpg"
       alt="Fig.12 scene setup"
       width="700">
  <figcaption><strong>Fig.12 scene setup</strong></figcaption>
</figure>

In this project, we designed and implemented a modular system for assistive and interactive cooking. By combining vision-language models (VLMs), large language models (LLMs), and multimodal intention detection (gaze and gesture), the system enables natural and flexible collaboration between humans and robots. It can understand spoken instructions, perceive the environment, and infer user intentions even without explicit text input, while executing complex cooking-related tasks through a hierarchical control pipeline. With a multi-camera setup and a library of fundamental robotic skills, the system demonstrates how generalization and context-aware interaction can be achieved in human-robot collaboration.

That being said, the current system faces challenges in handling ROS2 interaction inputs from different threads. Specifically, voice commands are received at a relatively low frequency, whereas gaze and gesture inputs are processed at a much higher frequency. At present, there is no optimal mechanism to allow any of these modalities to independently trigger intention detection. Within the scope of this project, voice commands are used exclusively as the trigger for intention recognition. In cases where the verbal input is ambiguous, gesture and gaze labels must be generated in advance to provide the necessary contextual information. Otherwise, the system may fail to respond appropriately due to the absence of object labels associated with the voice command. Additionaly, point cloud for (semi-)transparent objects is not accurate and cannot be optmized through preprocessing, which is also a constraint that prevents us from generalizing the grasping method to more various objects.

Looking ahead, there are several promising directions to extend this work. First, incorporating dynamic obstacle avoidance will improve safety in cluttered and changing environments. Second, adopting learning-based grasping strategies such as VGN, GPD, GIGA can replace handcrafted heuristics, enabling more robust manipulation of diverse kitchen objects. Third, deploying LLMs and VLMs locally will reduce latency, making the system more efficient and practical for real-world use. Finally, further VRAM optimization can broaden hardware compatibility and enhance responsiveness.

Overall, this work represents an important step toward interactive and assistive robotics in everyday life. It highlights how modular design, multimodal intention understanding, and LLM-driven decision-making can be integrated to create a flexible, generalizable, and user-friendly intelligent cooking assistant.

## **Reference**

[1] Wang H, Kedia K, Ren J, et al. MOSAIC: Modular Foundation Models for Assistive and Interactive Cooking[C]//8th Annual Conference on Robot Learning. 2024.

[2] Cao N, Lin Y R, Sun X, et al. Whisper: Tracing the spatiotemporal process of information diffusion in real time[J]. IEEE transactions on visualization and computer graphics, 2012, 18(12): 2649-2658.

[3] Abdelrahman A A, Hempel T, Khalifa A, et al. L2cs-net: Fine-grained gaze estimation in unconstrained environments[C]//2023 8th International Conference on Frontiers of Signal Processing (ICFSP). IEEE, 2023: 98-102.

[4] Heigold G, Minderer M, Gritsenko A, et al. Video owl-vit: Temporally-consistent open-world localization in video[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 13802-13811.

[5] Kirillov A, Mintun E, Ravi N, et al. Segment anything[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 4015-4026.

[6] Jegham N, Koh C Y, Abdelatti M, et al. Yolo evolution: A comprehensive benchmark and architectural review of yolov12, yolo11, and their previous versions[J]. arXiv preprint arXiv:2411.00201, 2024.

[7] Ten Pas A, Gualtieri M, Saenko K, Platt R. Grasp pose detection in point clouds[J]. The International Journal of Robotics Research, 2017, 36(13-14): 1455-1473.